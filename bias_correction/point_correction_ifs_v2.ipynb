{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49371b1",
   "metadata": {},
   "source": [
    "This notebook is an iteration of the IFS model to take advantage of presaved X and Y arrays to allow shuffling and multiple works in the training dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf56cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import copy\n",
    "import xarray as xr\n",
    "from scipy.spatial import KDTree\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import inv_boxcox\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "# from denseweight import DenseWeight\n",
    "# import smogn\n",
    "from collections import defaultdict\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.__version__)\n",
    "# from fwx.utilities import calc_windspeed\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import time\n",
    "import xarray as xr\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a200c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "import torch\n",
    "\n",
    "profiler = PyTorchProfiler(\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "        \"/gscratch/kylabazlen/ml_out/tb_profiler_logs\"  # No trailing space\n",
    "    ),\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "    profile_memory=True,\n",
    "    record_shapes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ec485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profiler.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cdcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check node\n",
    "import socket\n",
    "print(f\"Running on: {socket.gethostname()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a01bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {torch.cuda.device_count()} GPUs')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
    "# print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26877c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_windspeed(\n",
    "    wind_v: np.ndarray | xr.DataArray,\n",
    "    wind_u: np.ndarray | xr.DataArray,\n",
    ") -> np.ndarray | xr.DataArray:\n",
    "    \"\"\"\n",
    "    Compute wind speed from eastward (u) and northward (v) wind velocities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wind_u: np.ndarray | xr.DataArray\n",
    "        east (+) west (-) wind velocity (m/s)\n",
    "    wind_v: np.ndarray | xr.DataArray\n",
    "        north (+) south (-) wind velocity (m/s)\n",
    "\n",
    "    Return\n",
    "    ---------\n",
    "    : np.ndarray | xr.DataArray\n",
    "        Wind speed (m/s)\n",
    "    \"\"\"\n",
    "    assert wind_v.shape == wind_u.shape, \"arguments have mismatched shape\"\n",
    "    return np.hypot(wind_u, wind_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_time_latlon(ds: xr.Dataset,\n",
    "                        qc_ds: xr.Dataset,\n",
    "                )-> tuple[xr.Dataset, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Intersect two datasets on both time and exact (latitude, longitude) pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        First dataset with dims (time, space) and coordinates (latitude, longitude).\n",
    "    qc_ds : xr.Dataset\n",
    "        Second dataset with dims (time, space) and coordinates (latitude, longitude).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (xr.Dataset, xr.Dataset)\n",
    "        Subsetted datasets with only common times and lat, lons.\n",
    "    \"\"\"\n",
    "    common_time = np.intersect1d(ds.time.values, qc_ds.time.values)\n",
    "    ds = ds.sel(time=common_time)\n",
    "    qc_ds = qc_ds.sel(time=common_time)\n",
    "\n",
    "    #get lat lon pairs\n",
    "    coords_ds = set(zip(ds.latitude.values, ds.longitude.values))\n",
    "    coords_qc = set(zip(qc_ds.latitude.values, qc_ds.longitude.values))\n",
    "    common_coords = coords_ds & coords_qc\n",
    "\n",
    "    #Masks\n",
    "    mask_ds = [(lat, lon) in common_coords for lat, lon in zip(ds.latitude.values, ds.longitude.values)]\n",
    "    mask_qc = [(lat, lon) in common_coords for lat, lon in zip(qc_ds.latitude.values, qc_ds.longitude.values)]\n",
    "\n",
    "    #subset datasets by index\n",
    "    ds = ds.isel(space=np.array(mask_ds))\n",
    "    qc_ds = qc_ds.isel(space=np.array(mask_qc))\n",
    "\n",
    "    return ds, qc_ds\n",
    "\n",
    "def compute_qc_fail_mask(dset_qc: xr.Dataset, bits_to_mask: list[int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute boolean QC failure mask for a QC dataset.\n",
    "\n",
    "    Returns True where any specified bits fail for any variable.\n",
    "    \"\"\"\n",
    "    bits = [b for b in bits_to_mask if b != 1]\n",
    "    mask = sum(1 << (b - 1) for b in bits)\n",
    "\n",
    "    combined_fails = None\n",
    "    for var in dset_qc.data_vars:\n",
    "        qc_values = np.nan_to_num(dset_qc[var].values, nan=0).astype(\"int32\")\n",
    "        # qc_values = dset_qc[var].fillna(0).astype(\"int32\") #maybe faster? idk \n",
    "        fails = (qc_values & mask) != 0\n",
    "        combined_fails = (\n",
    "            fails if combined_fails is None else (combined_fails | fails)\n",
    "        )\n",
    "\n",
    "    return combined_fails\n",
    "\n",
    "def qc_threshold(ds: xr.Dataset, \n",
    "                 threshold_variable: list[str],\n",
    "                 threshold: int,\n",
    "                 mask_variable: list[str] = None):\n",
    "\n",
    "\n",
    "    if mask_variable is None:\n",
    "        mask_variable = threshold_variable\n",
    "\n",
    "    combined_mask = None\n",
    "    for var in threshold_variable:\n",
    "        mask = ds[var] > threshold\n",
    "        combined_mask = mask if combined_mask is None else (combined_mask | mask)\n",
    "\n",
    "    for mvar in mask_variable:\n",
    "        if mvar in ds:\n",
    "             ds[mvar] = ds[mvar].where(~combined_mask)\n",
    "\n",
    "    return ds\n",
    "#spatial checkerboard of cowy into train and testing datasets\n",
    "def spatial_blocking(\n",
    "    cowy: \"CoWyPointDataset\",\n",
    "    block_size: float,\n",
    "    n_folds: int,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Split dataset into spatially blocked, checkerboard train and test datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cowy : \n",
    "        Pytorch object... \n",
    "    block_size : float, optional\n",
    "        Size of spatial blocks in coordinate units (e.g., degrees).\n",
    "    n_folds : int, optional\n",
    "        Number of spatial folds, 2 folds results in equall allocation of\n",
    "        checkers to test and train datasets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ds_train : xr.Dataset\n",
    "        Subset of the original dataset containing training locations.\n",
    "    ds_test : xr.Dataset\n",
    "        Subset of the original dataset containing testing locations.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract coordinate arrays\n",
    "    lat = cowy.lat_obs\n",
    "    lon = cowy.lon_obs\n",
    "\n",
    "    # Anchor the grid at the dataset's min lat/lon (ensures reproducibility)\n",
    "    lat_min = float(np.floor(lat.min()))\n",
    "    lon_min = float(np.floor(lon.min()))\n",
    "\n",
    "    # Compute block indices\n",
    "    block_y = np.floor((lat - lat_min) / block_size)\n",
    "    block_x = np.floor((lon - lon_min) / block_size)\n",
    "\n",
    "    # Compute fold IDs (checkerboard assignment)\n",
    "    fold_id = (block_x + block_y) % n_folds\n",
    "\n",
    "    # Create masks\n",
    "    train_mask = fold_id != 0\n",
    "    test_mask = fold_id == 0\n",
    "    # Convert masks to indices\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "\n",
    "    # Subset dataset\n",
    "    train_ds = copy.copy(cowy) #shallow copy\n",
    "    \n",
    "    train_ds.obs_lookup = cowy.obs_lookup[\n",
    "        cowy.obs_lookup['idx_obs'].isin(train_idx)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    test_ds = copy.copy(cowy)\n",
    "    test_ds.obs_lookup = cowy.obs_lookup[cowy.obs_lookup['idx_obs'].isin(test_idx)].reset_index(drop=True)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "#not used\n",
    "def add_raster_variable(\n",
    "    ds: xr.Dataset,\n",
    "    path: str,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Add variable from .nc file with coordinates latitude and longitude.\n",
    "\n",
    "    Performs vectorized nearest-neighbor lookup to extract values from a\n",
    "    gridded raster dataset at each meteorological station location.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Dataset containing latitude and longitude of met station\n",
    "        indexed by space.\n",
    "    path : str\n",
    "        Path to variable netcdf file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Dataset with an added variable, by name variable_name_out.\n",
    "    \"\"\"\n",
    "\n",
    "    raster_ds = xr.open_mfdataset(path, compat='no_conflicts')\n",
    "\n",
    "    for var_name in raster_ds.data_vars:\n",
    "        vals = raster_ds[var_name].interp(\n",
    "            latitude=ds[\"latitude\"],\n",
    "            longitude=ds[\"longitude\"],\n",
    "            method=\"nearest\",\n",
    "        )\n",
    "        ds[var_name] = vals\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hypsometric_equation(p1, p2, t1, t2):\n",
    "    r = 287 #specific gas constant J/kg/K\n",
    "    g = 9.81 #m/s2\n",
    "    t_mean = (t1+t2)/2\n",
    "    return (r * t_mean) /g * np.log(p1 / p2)\n",
    "\n",
    "def compute_elr(p_surface, t_surface, p_upper, t_upper, z_surface=2.0):\n",
    "    \"\"\"\n",
    "    Compute environmental lapse rate between surface and an upper level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_surface : float or array\n",
    "        Surface pressure (Pa).\n",
    "    t_surface : float or array\n",
    "        Surface temperature (K).\n",
    "    p_upper : float or array\n",
    "        Upper-level pressure (Pa).\n",
    "    t_upper : float or array\n",
    "        Upper-level temperature (K).\n",
    "    z_surface : float, optional\n",
    "        Surface height (m). Default is 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    elr : float or array\n",
    "        Lapse rate (K/m).\n",
    "    \"\"\"\n",
    "    # Height difference using hypsometric equation\n",
    "    dz = _hypsometric_equation(p_surface, p_upper, t_surface, t_upper)\n",
    "    z_upper = z_surface + dz\n",
    "\n",
    "    # Lapse rate K/m\n",
    "    elr = (t_upper - t_surface) / dz\n",
    "    return elr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoWyPointDataset(Dataset):\n",
    "    \"\"\"This object returns a single observation from the training or validation data.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 source_madis_fps,\n",
    "                 source_hrrr_fps,\n",
    "                 source_topo_fps,\n",
    "                 dist_lim = 0.25, #dist_lim=0.027,\n",
    "                 shuffle=True,\n",
    "                 obs_meta_cache = None):\n",
    "        \"\"\"\n",
    "        ***Shuffle here and NOT in the DataLoader object!!!***\n",
    "        ***Only use num_workers=0 with the DataLoader object!!!***\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dist_lim : float\n",
    "            ~ 3km in decimal degrees = 0.027\n",
    "            ~ 0.25 degrees is IFS resolution ~ 25km\n",
    "        shuffle : bool\n",
    "            Flag to shuffle timesteps\n",
    "        \"\"\"\n",
    "\n",
    "        self.dset_madis_open = xr.open_dataset(source_madis_fps, engine='h5netcdf')\n",
    "        self.dset_madis = self.dset_madis_open[['windspeed_10m']]\n",
    "\n",
    "        #read in hrrr removing step attirbute\n",
    "        self.dsets_hrrr = {}\n",
    "        for fp in source_hrrr_fps:\n",
    "            ds = xr.open_dataset(fp, decode_cf=False)\n",
    "            if \"step\" in ds and \"dtype\" in ds[\"step\"].attrs:\n",
    "                ds[\"step\"].attrs.pop(\"dtype\")\n",
    "            ds = xr.decode_cf(ds)\n",
    "            ds['ws_10'] = calc_windspeed(wind_v=ds['v10'], wind_u=ds['u10'])\n",
    "\n",
    "            # Convert step from timedelta to hours and add as a variable\n",
    "            step_hours = ds.step.values / np.timedelta64(1, 'h')\n",
    "            ds['lead_time_hrs'] = xr.DataArray(\n",
    "            np.full((ds.dims['latitude'], ds.dims['longitude']), step_hours, dtype='float32'),\n",
    "            dims=['latitude', 'longitude']\n",
    "            )\n",
    "\n",
    "            #encode initialization time as 0 or 1\n",
    "            init_hour = pd.Timestamp(ds['time'].values).hour\n",
    "            init_z = 1.0 if init_hour == 12 else 0.0\n",
    "            ds['init_z'] = xr.DataArray(\n",
    "            np.full((ds.dims['latitude'], ds.dims['longitude']), init_z, dtype='float32'),\n",
    "            dims=['latitude', 'longitude']\n",
    "            )\n",
    "\n",
    "            #remove \"surface\"\n",
    "            ds = ds.drop_vars(\"surface\", errors=\"ignore\")\n",
    "\n",
    "            # Compute ELR\n",
    "            p_surface = ds[\"sp\"].astype(\"float32\")\n",
    "            t_surface = ds[\"t2m\"].astype(\"float32\")\n",
    "            p_500 = 50000.0\n",
    "            t_500 = ds[\"t_500hPa\"].astype(\"float32\")\n",
    "\n",
    "            ds[\"elr\"] = compute_elr(\n",
    "                p_surface=p_surface,\n",
    "                t_surface=t_surface,\n",
    "                p_upper=p_500,\n",
    "                t_upper=t_500,\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "            # Add cyclical encoding of month and day\n",
    "                # adds 4 variables (hour_sin, hour_cos, day_sin, day_cos)\n",
    "            # ds = self.encode_time(ds)\n",
    "            # ds = ds.drop_vars([\"time\"], errors=\"ignore\")\n",
    "\n",
    "            #1 D lat lon\n",
    "            lat = ds['latitude'].values\n",
    "            lon = ds['longitude'].values\n",
    "\n",
    "            # Create 2D meshgrid for latitude and longitude\n",
    "            lat_2d, lon_2d = np.meshgrid(lat, lon, indexing='ij')  #shape (y, x)\n",
    "\n",
    "            # Add the 2D latitude and longitude to the dataset as coordinates\n",
    "            ds = ds.assign_coords({\n",
    "                \"latitude\": ((\"y\", \"x\"), lat_2d),\n",
    "                \"longitude\": ((\"y\", \"x\"), lon_2d)\n",
    "            })\n",
    "\n",
    "            # Reindex the dataset to use x and y as dimensions\n",
    "            ds = ds.swap_dims({\"latitude\": \"y\", \"longitude\": \"x\"})  # Swap dimensions\n",
    "\n",
    "            # Drop the original 1D latitude and longitude dimensions\n",
    "            ds = ds.drop_dims([\"lat\", \"lon\"], errors=\"ignore\")\n",
    "\n",
    "            self.dsets_hrrr[fp] = ds\n",
    "\n",
    "        #only keep over lapping time steps.\n",
    "\n",
    "        #read in terrain data\n",
    "        self.dset_topo = xr.open_mfdataset(\n",
    "            source_topo_fps,\n",
    "            chunks={'latitude': 1000, 'longitude': 1000},\n",
    "            engine=\"h5netcdf\"\n",
    "            )\n",
    "\n",
    "        self.vars_topo = [v for v in self.dset_topo.data_vars if self.dset_topo[v].ndim == 2]\n",
    "\n",
    "        self.dist_lim = dist_lim\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # self.hrrr_madis_cache_fp = hrrr_madis_cache_fp\n",
    "        self.cache_madis = None\n",
    "        self.cache_hrrr = None\n",
    "        self.cache_timestamp = None\n",
    "\n",
    "        # sample hrrr single timestep for meta data calculation\n",
    "        dset_hrrr = list(self.dsets_hrrr.values())[0]\n",
    "\n",
    "        # \"valid_time\" timestamp to hrrr filepath lookup\n",
    "        self.times_hrrr = {pd.to_datetime(dset['valid_time'].values): fp \n",
    "                           for fp, dset in self.dsets_hrrr.items()}\n",
    "\n",
    "        ## CHANGE TO THIS AND UPDATE ALL OTHER CODE TO ACCEPT A LIST OF FILE PATHS!!!\n",
    "        # \"valid_time\" timestamp to hrrr filepath lookup (multiple files per time)\n",
    "        #     #multipl files per valid time for working with multiple forecast lead times of ifs\n",
    "        # self.times_hrrr = defaultdict(list)\n",
    "        # for fp, dset in self.dsets_hrrr.items():\n",
    "        #     vt = pd.to_datetime(dset[\"valid_time\"].values)\n",
    "        #     self.times_hrrr[vt].append(fp)\n",
    "\n",
    "        #filter madis to only the valid times present in forecasts\n",
    "        valid_times = np.array(list(self.times_hrrr.keys()), dtype=\"datetime64[ns]\")\n",
    "        mask = np.isin(self.dset_madis[\"time\"].values.astype(\"datetime64[ns]\"), valid_times)\n",
    "        self.dset_madis = self.dset_madis.isel(time=mask)\n",
    "        self.ti_madis = pd.to_datetime(self.dset_madis[\"time\"].values)\n",
    "\n",
    "        # self.ti_madis = pd.to_datetime(self.dset_madis['time'])\n",
    "        self.vars_hrrr = list(dset_hrrr.keys())\n",
    "        self.vars_madis = list(self.dset_madis.keys())\n",
    "\n",
    "        #load madis into memory once!!!\n",
    "        self.madis_arrays = {}\n",
    "        for var in self.vars_madis:\n",
    "            self.madis_arrays[var] = self.dset_madis[var].values\n",
    "\n",
    "        self.lat_obs = self.dset_madis['latitude'].values\n",
    "        self.lon_obs = self.dset_madis['longitude'].values\n",
    "        self.lat_hrrr = dset_hrrr['latitude'].values\n",
    "        self.lon_hrrr = dset_hrrr['longitude'].values\n",
    "        #topo originally comes as 1D lat/lon\n",
    "        lat1d_topo = self.dset_topo['latitude'].values\n",
    "        lon1d_topo = self.dset_topo['longitude'].values\n",
    "        #make topo lat lon 2d to match grid (and hrrr workflow which comes in x,y)\n",
    "        self.lon_topo, self.lat_topo = np.meshgrid(lon1d_topo, lat1d_topo)\n",
    "        self.shape_topo = self.lat_topo.shape\n",
    "\n",
    "        coords_hrrr = (self.lat_hrrr.flatten(), self.lon_hrrr.flatten())\n",
    "        coords_hrrr = np.vstack(coords_hrrr).T\n",
    "        coords_obs = (self.lat_obs, self.lon_obs)\n",
    "        coords_obs = np.vstack(coords_obs).T\n",
    "\n",
    "        # self.nn_ind will give you the HRRR index for a requesting obs index\n",
    "        self.tree = KDTree(coords_hrrr)\n",
    "        self.nn_dist, self.nn_ind = self.tree.query(coords_obs)\n",
    "\n",
    "        self.size_hrrr = self.lat_hrrr.size\n",
    "        self.shape_hrrr = self.lat_hrrr.shape\n",
    "        self.ind_hrrr = np.arange(self.size_hrrr)\n",
    "        self.ind_hrrr = self.ind_hrrr.reshape(self.shape_hrrr)\n",
    "\n",
    "        #kdtree for topo data to match grid to point obs\n",
    "        coords_topo = (self.lat_topo.flatten(), self.lon_topo.flatten()) #coordinates at grid cells x,y\n",
    "        coords_topo = np.vstack(coords_topo).T\n",
    "\n",
    "        self.tree_topo = KDTree(coords_topo) #give index of nearest gridcell \n",
    "        self.nn_dist_topo, self.nn_ind_topo = self.tree_topo.query(coords_obs)\n",
    "        \n",
    "        #splitting flat index from kdtree into x and y indexs\n",
    "        self.idy_topo_pre, self.idx_topo_pre = np.unravel_index(\n",
    "            self.nn_ind_topo, self.shape_topo\n",
    "        )\n",
    "        #convert index into int32\n",
    "        self.idy_topo_pre = self.idy_topo_pre.astype(np.int32)\n",
    "        self.idx_topo_pre = self.idx_topo_pre.astype(np.int32)\n",
    "        #wrap indexes in data array objects\n",
    "        idy = xr.DataArray(self.idy_topo_pre, dims=\"nobs\")\n",
    "        idx = xr.DataArray(self.idx_topo_pre, dims=\"nobs\")\n",
    "\n",
    "        topo_list = []\n",
    "        #loop through each topo variable and select value at nearest grid cell\n",
    "        for v in self.vars_topo:\n",
    "            topo_list.append(self.dset_topo[v].isel(latitude=idy, longitude=idx))\n",
    "        print(\"end topo list\")\n",
    "        #cmobine topo features into per observation array\n",
    "        self.topo_per_obs = (\n",
    "            xr.concat(topo_list, dim=\"var\")\n",
    "            .compute()\n",
    "            .to_numpy()\n",
    "            .T\n",
    "            .astype(np.float32)\n",
    "        )\n",
    "\n",
    "        self.idx_obs_inbounds = np.where(self.nn_dist < self.dist_lim)[0]\n",
    "\n",
    "        self.obs_lookup = None\n",
    "\n",
    "        if obs_meta_cache is None:\n",
    "            self.set_obs_lookup()\n",
    "        elif isinstance(obs_meta_cache, str):\n",
    "            df = pd.read_csv(obs_meta_cache)\n",
    "            valid_fps = set(source_hrrr_fps)\n",
    "            df = df[df['fp_hrrr'].isin(valid_fps)].reset_index(drop=True)\n",
    "\n",
    "            # column types to match what set_obs_lookup() produces\n",
    "            df['idx_obs'] = df['idx_obs'].astype(np.int32)\n",
    "            df['idt_madis'] = df['idt_madis'].astype(np.int32)\n",
    "            df['idy_hrrr'] = df['idy_hrrr'].astype(np.int32)\n",
    "            df['idx_hrrr'] = df['idx_hrrr'].astype(np.int32)\n",
    "            df['idy_topo'] = df['idy_topo'].astype(np.int32)\n",
    "            df['idx_topo'] = df['idx_topo'].astype(np.int32)\n",
    "            df['latitude'] = df['latitude'].astype(np.float32)\n",
    "            df['longitude'] = df['longitude'].astype(np.float32)\n",
    "\n",
    "            #convert timestampsto pandas.Timestamp\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "            self.obs_lookup = df\n",
    "\n",
    "        #add obs number column\n",
    "        self.obs_lookup['obs_number'] = np.arange(len(self.obs_lookup))\n",
    "\n",
    "    def encode_time(self,\n",
    "                    ds: xr.Dataset) -> xr.Dataset:\n",
    "        \"\"\"Add cyclical time encodings (hour and day of year) as 2D spatial grids.\"\"\"\n",
    "        \n",
    "        # Get shape and dims from first 2D variable\n",
    "        spatial_vars = [v for v in ds.data_vars if ds[v].ndim == 2]\n",
    "        if not spatial_vars:\n",
    "            raise ValueError(\"No 2D variables found in dataset\")\n",
    "        \n",
    "        sample_var = ds[spatial_vars[0]]\n",
    "        ny, nx = sample_var.shape\n",
    "        spatial_dims = list(sample_var.dims)\n",
    "    \n",
    "        # Extract time\n",
    "        timestamp = pd.to_datetime(ds['time'].values)\n",
    "        hour = timestamp.hour\n",
    "        doy = timestamp.timetuple().tm_yday\n",
    "        \n",
    "        # Add cyclical encodings (sin/cos pairs preserve periodicity)\n",
    "        ds['hour_sin'] = (spatial_dims, np.full((ny, nx), np.sin(2*np.pi*hour/24), dtype=np.float32))\n",
    "        ds['hour_cos'] = (spatial_dims, np.full((ny, nx), np.cos(2*np.pi*hour/24), dtype=np.float32))\n",
    "        ds['doy_sin'] = (spatial_dims, np.full((ny, nx), np.sin(2*np.pi*doy/366), dtype=np.float32))\n",
    "        ds['doy_cos'] = (spatial_dims, np.full((ny, nx), np.cos(2*np.pi*doy/366), dtype=np.float32))\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def set_obs_lookup(self): #new build set obs lookup collecting -- adds to the table once per timestep not once per obs\n",
    "        \"\"\"Set all of the meta data for each observation index so you can quickly \n",
    "        look up timestep and corresponding HRRR file\n",
    "\n",
    "        This can take longer for full years of data with lots of obs\n",
    "        \"\"\"\n",
    "        time_index = list(self.ti_madis.copy())\n",
    "\n",
    "        #initialize lists\n",
    "        data = {\n",
    "            'idx_obs': [],\n",
    "            'idt_madis': [],\n",
    "            'timestamp': [],\n",
    "            'fp_hrrr': [],\n",
    "            'idy_hrrr': [],\n",
    "            'idx_hrrr': [],\n",
    "            'idy_topo': [],\n",
    "            'idx_topo': [],\n",
    "            'latitude':[],\n",
    "            'longitude':[],\n",
    "        }\n",
    "\n",
    "        for idt, timestamp in enumerate(time_index):\n",
    "            idt_madis = np.where(self.ti_madis == timestamp)[0][0]\n",
    "            fp_hrrr = self.times_hrrr.get(timestamp, None)\n",
    "\n",
    "            if fp_hrrr is not None:\n",
    "                obs_arr_ws = self.madis_arrays['windspeed_10m'][idt_madis]\n",
    "                idx_obs_notnan = np.where(~np.isnan(obs_arr_ws))[0]\n",
    "                idx_obs = sorted(set(idx_obs_notnan).intersection(set(self.idx_obs_inbounds)))\n",
    "                \n",
    "                if len(idx_obs) == 0:\n",
    "                    continue\n",
    "                \n",
    "                idx_obs_arr = np.array(idx_obs)\n",
    "                n_obs = len(idx_obs_arr)\n",
    "                \n",
    "                hrrr_positions = np.array([np.where(self.ind_hrrr == self.nn_ind[idx]) \n",
    "                                        for idx in idx_obs_arr])\n",
    "                idy_hrrr_arr = hrrr_positions[:, 0, 0]\n",
    "                idx_hrrr_arr = hrrr_positions[:, 1, 0]\n",
    "\n",
    "                idy_topo_arr = np.array([self.idy_topo_pre[idx] for idx in idx_obs_arr])\n",
    "                idx_topo_arr = np.array([self.idx_topo_pre[idx] for idx in idx_obs_arr])\n",
    "\n",
    "                # #append data\n",
    "                # data['idx_obs'].extend(idx_obs_arr)\n",
    "                # data['idt_madis'].extend([idt_madis] * n_obs)\n",
    "                # data['timestamp'].extend([timestamp] * n_obs)\n",
    "                # data['fp_hrrr'].extend([fp_hrrr] * n_obs)\n",
    "                # data['idy_hrrr'].extend(idy_hrrr_arr)\n",
    "                # data['idx_hrrr'].extend(idx_hrrr_arr)\n",
    "                # data['idy_topo'].extend(idy_topo_arr)\n",
    "                # data['idx_topo'].extend(idx_topo_arr)\n",
    "                # data['latitude'].extend(self.lat_obs[idx_obs])\n",
    "                # data['longitude']. extend(self.lon_obs[idx_obs])\n",
    "\n",
    "                #look over hrrr files for each valid time.\n",
    "                if not isinstance(fp_hrrr, list):\n",
    "                    fp_hrrr = [fp_hrrr]\n",
    "                for fp in fp_hrrr:\n",
    "                    data['idx_obs'].extend(idx_obs_arr)\n",
    "                    data['idt_madis'].extend([idt_madis] * n_obs)\n",
    "                    data['timestamp'].extend([timestamp] * n_obs)\n",
    "                    data['fp_hrrr'].extend([fp] * n_obs)\n",
    "                    data['idy_hrrr'].extend(idy_hrrr_arr)\n",
    "                    data['idx_hrrr'].extend(idx_hrrr_arr)\n",
    "                    data['idy_topo'].extend(idy_topo_arr)\n",
    "                    data['idx_topo'].extend(idx_topo_arr)\n",
    "                    data['latitude'].extend(self.lat_obs[idx_obs])\n",
    "                    data['longitude'].extend(self.lon_obs[idx_obs])\n",
    "        \n",
    "        # Create DataFrame from complete data\n",
    "        self.obs_lookup = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert types\n",
    "        for col in ['idx_obs', 'idt_madis', 'idy_hrrr', 'idx_hrrr']:\n",
    "            self.obs_lookup[col] = self.obs_lookup[col].astype(int)\n",
    "\n",
    "    def shuffle_timesteps(self):\n",
    "        \"\"\"Shuffle obs lookup while keeping timesteps together for efficient IO\"\"\"\n",
    "        random_order = pd.Series(\n",
    "            np.random.permutation(self.obs_lookup['timestamp'].unique()),\n",
    "            index=self.obs_lookup['timestamp'].unique()\n",
    "        )\n",
    "        \n",
    "        self.obs_lookup['random_order'] = self.obs_lookup['timestamp'].map(random_order)\n",
    "        \n",
    "        self.obs_lookup = self.obs_lookup.sort_values('random_order')\n",
    "        self.obs_lookup = self.obs_lookup.drop('random_order', axis=1)\n",
    "        self.obs_lookup = self.obs_lookup.reset_index(drop=True)\n",
    "\n",
    "    def get_cached_arrs(self, idt_madis, timestamp):\n",
    "        \"\"\"Get an single timestep of HRRR and Obs data cached as numpy array \n",
    "\n",
    "        Because we group all spatial obs within a given time index its more \n",
    "        efficient to cache all data for that time index as numpy array and \n",
    "        iterate through that before the next time index is requested\n",
    "        \"\"\"\n",
    "        fp_hrrr = self.times_hrrr.get(timestamp, None)\n",
    "        dset_hrrr = self.dsets_hrrr[fp_hrrr]\n",
    "\n",
    "        # load first timestep into memory\n",
    "        if self.cache_madis is None or self.cache_hrrr is None:\n",
    "            self.cache_timestamp = dset_hrrr['valid_time'].values.copy()\n",
    "            self.cache_hrrr = [dset_hrrr[var].values for var in self.vars_hrrr]\n",
    "            self.cache_hrrr = np.dstack(self.cache_hrrr)\n",
    "            # self.cache_madis = [self.dset_madis.isel(time=idt_madis)[var].values for var in self.vars_madis]\n",
    "            self.cache_madis = [self.madis_arrays[var][idt_madis] for var in self.vars_madis]\n",
    "            self.cache_madis = np.vstack(self.cache_madis).T\n",
    "            self.cache_hrrr = self.cache_hrrr.astype(np.float32)\n",
    "            self.cache_madis = self.cache_madis.astype(np.float32)\n",
    "\n",
    "        # New timestep, load into memory\n",
    "        if self.cache_timestamp != dset_hrrr['valid_time'].values:\n",
    "            self.cache_timestamp = dset_hrrr['valid_time'].values.copy()\n",
    "            self.cache_hrrr = [dset_hrrr[var].values for var in self.vars_hrrr]\n",
    "            self.cache_hrrr = np.dstack(self.cache_hrrr)\n",
    "            # self.cache_madis = [self.dset_madis.isel(time=idt_madis)[var].values for var in self.vars_madis]\n",
    "            self.cache_madis = [self.madis_arrays[var][idt_madis] for var in self.vars_madis]\n",
    "            self.cache_madis = np.vstack(self.cache_madis).T\n",
    "            self.cache_hrrr = self.cache_hrrr.astype(np.float32)\n",
    "            self.cache_madis = self.cache_madis.astype(np.float32)\n",
    "    \n",
    "        return self.cache_hrrr, self.cache_madis\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of observations per epoch\"\"\"\n",
    "        return len(self.obs_lookup)\n",
    "\n",
    "    def __getitem__(self, idx, check_coords=True):\n",
    "        \"\"\"This returns the single observation inputs and target numpy array.\n",
    "        Must be float32\n",
    "        \"\"\"\n",
    "\n",
    "        if idx == 0 and self.shuffle:\n",
    "            self.shuffle_timesteps()\n",
    "\n",
    "        idx_obs = self.obs_lookup.at[idx, 'idx_obs']\n",
    "        timestamp = self.obs_lookup.at[idx, 'timestamp']\n",
    "        idt_madis = self.obs_lookup.at[idx, 'idt_madis']\n",
    "        idy_hrrr = self.obs_lookup.at[idx, 'idy_hrrr']\n",
    "        idx_hrrr = self.obs_lookup.at[idx, 'idx_hrrr']\n",
    "        \n",
    "        idt_arr_hrrr, idt_arr_madis = self.get_cached_arrs(idt_madis, timestamp)\n",
    "\n",
    "        if check_coords:\n",
    "            assert np.abs(self.lat_obs[idx_obs] - self.lat_hrrr[idy_hrrr, idx_hrrr]) < self.dist_lim\n",
    "            assert np.abs(self.lon_obs[idx_obs] - self.lon_hrrr[idy_hrrr, idx_hrrr]) < self.dist_lim\n",
    "        \n",
    "        hrrr_inputs = idt_arr_hrrr[idy_hrrr, idx_hrrr]\n",
    "        topo_inputs = self.topo_per_obs[idx_obs]\n",
    "        inputs = np.concatenate([hrrr_inputs, topo_inputs]).astype(np.float32)\n",
    "\n",
    "        target = idt_arr_madis[idx_obs]\n",
    "        if np.isnan(target).any():\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0741a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_madis_fps = '/project/cowy-nvhackathon/cowy-wildfire/data/observations/cowy_madis_metar_mesonet_2024.nc'\n",
    "source_hrrr_fps = sorted(glob.glob('/project/cowy-nvhackathon/cowy-wildfire/data/nwp/ifs/*.nc'))\n",
    "source_topo_fps  = '/project/cowy-nvhackathon/cowy-wildfire/data/terrain_data/terrain_990m/*_reprojected_wgs84_cowy_990m.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf991ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_lookup = '/gscratch/kylabazlen/ml_out/test_2/obs_lookup.csv'\n",
    "cowy = CoWyPointDataset(source_madis_fps,\n",
    "                        source_hrrr_fps,\n",
    "                        source_topo_fps,\n",
    "                        obs_meta_cache=None)\n",
    "\n",
    "cowy.obs_lookup.to_csv('/gscratch/kylabazlen/ml_out/test_2/obs_lookup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ade8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ONCE\n",
    "#run onces to get x and y arrays saved\n",
    "X_all = []\n",
    "Y_all = []\n",
    "\n",
    "#builds x and y in the order of obs lookup\n",
    "# Result: X_all[i] and Y_all[i] correspond to obs_lookup.iloc[i]\n",
    "\n",
    "for i in range(len(cowy)):\n",
    "    x, y = cowy[i]\n",
    "    X_all.append(x)\n",
    "    Y_all.append(y)\n",
    "\n",
    "# More explicit stacking\n",
    "X_all = np.vstack(X_all).astype(np.float32)  # shape: (n_samples, n_features)\n",
    "Y_all = np.array(Y_all, dtype=np.float32)    # shape: (n_samples,) if y is scalar\n",
    "\n",
    "mean = np.nanmean(X_all, axis = 0)\n",
    "sd = np.nanstd(X_all, axis = 0)\n",
    "\n",
    "# OR\n",
    "# Y_all = np.vstack(Y_all).astype(np.float32)  # shape: (n_samples, 1) for 2d\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/mean.npy\", mean)\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/sd.npy\", sd)\n",
    "\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/X_all.npy\", X_all)\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/Y_all.npy\", Y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b17a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_train_classes(train_ds, thresholds=[11.17]):\n",
    "    \"\"\"\n",
    "    Balance training dataset by upsampling minority classes to match majority class.\n",
    "    \n",
    "    Bins observations into wind speed ranges and upsamples each bin to match\n",
    "    the size of the largest bin (typically low wind speeds).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    train_ds : CoWyPointDataset\n",
    "        The training dataset object containing the observation table (obs_lookup).\n",
    "    thresholds : list[float]\n",
    "        Thresholds defining wind speed bins. E.g., [5, 10] creates bins:\n",
    "        [0-5), [5-10), [10+) 11.17 = 25mph = ws for red flag warning.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    train_ds : CoWyPointDataset\n",
    "        The updated training dataset with balanced obs_lookup table.\n",
    "    \"\"\"\n",
    "    print(\"Starting class balancing on training data...\")\n",
    "    \n",
    "    obs_lookup = train_ds.obs_lookup.copy()\n",
    "    print(f\"Initial training obs_lookup size: {len(obs_lookup)}\")\n",
    "    \n",
    "    # Get target variable (observed wind speed)\n",
    "    obs_ws_all = train_ds.dset_madis[\"windspeed_10m\"].values\n",
    "    obs_ws = obs_ws_all[obs_lookup[\"idt_madis\"].values, obs_lookup[\"idx_obs\"].values]\n",
    "    \n",
    "    # Define bin edges\n",
    "    thresholds = sorted(thresholds)\n",
    "    edges = [0] + thresholds + [np.inf]\n",
    "    \n",
    "    # Separate data into bins\n",
    "    bins = {}\n",
    "    bin_sizes = {}\n",
    "    \n",
    "    for i, (lo, hi) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "        mask = (obs_ws >= lo) & (obs_ws < hi)\n",
    "        bin_samples = obs_lookup[mask]\n",
    "        label = f\"{lo}-{hi}\" if hi != np.inf else f\">{lo}\"\n",
    "        \n",
    "        bins[label] = bin_samples\n",
    "        bin_sizes[label] = len(bin_samples)\n",
    "        print(f\"Bin {label}: {len(bin_samples)} samples\")\n",
    "    \n",
    "    # Find maximum bin size (typically low wind speeds)\n",
    "    max_size = max(bin_sizes.values())\n",
    "    print(f\"\\nTarget size for all bins: {max_size}\")\n",
    "    \n",
    "    # Upsample each bin to match max_size\n",
    "    balanced_bins = []\n",
    "    for label, bin_samples in bins.items():\n",
    "        current_size = len(bin_samples)\n",
    "        \n",
    "        if current_size == 0:\n",
    "            print(f\"Warning: Bin {label} is empty, skipping\")\n",
    "            continue\n",
    "        \n",
    "        if current_size < max_size:\n",
    "            # Randomly sample with replacement to reach max_size\n",
    "            upsampled = bin_samples.sample(\n",
    "                n=max_size,\n",
    "                replace=True,\n",
    "                random_state=42\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"Bin {label}: upsampled from {current_size} to {len(upsampled)}\")\n",
    "        else:\n",
    "            # Already at max (this is the majority class)\n",
    "            upsampled = bin_samples.reset_index(drop=True)\n",
    "            print(f\"Bin {label}: kept at {len(upsampled)}\")\n",
    "        \n",
    "        balanced_bins.append(upsampled)\n",
    "    \n",
    "    # Combine all balanced bins\n",
    "    updated_obs_lookup = pd.concat(balanced_bins, ignore_index=True)\n",
    "    \n",
    "    # Optional: shuffle for better training dynamics\n",
    "    updated_obs_lookup = updated_obs_lookup.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nUpdated training obs_lookup size: {len(updated_obs_lookup)}\")\n",
    "    print(f\"Expansion factor: {len(updated_obs_lookup) / len(obs_lookup):.2f}x\")\n",
    "    \n",
    "    # Update the training dataset's obs_lookup table\n",
    "    train_ds.obs_lookup = updated_obs_lookup\n",
    "    \n",
    "    print(\"Class balancing completed.\")\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute mean and std on balanced training set\n",
    "X_balanced = []\n",
    "Y_balanced = []\n",
    "for i in range(len(train_ds)):\n",
    "    x, y = train_ds[i]\n",
    "    X_balanced.append(x)\n",
    "    Y_balanced.append(y)\n",
    "\n",
    "X_balanced = np.vstack(X_balanced).astype(np.float32)\n",
    "Y_balanced = np.array(Y_balanced, dtype=np.float32)\n",
    "\n",
    "mean_balanced = np.nanmean(X_balanced, axis=0)\n",
    "std_balanced = np.nanstd(X_balanced, axis=0)\n",
    "\n",
    "# Save or use these for normalization\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/mean_balanced.npy\", mean_balanced)\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/std_balanced.npy\", std_balanced)\n",
    "\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/X_all_balanced.npy\", X_balanced)\n",
    "np.save(\"/gscratch/kylabazlen/ml_out/test_1/Y_all_balanced.npy\", Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2° x 2° spatial blocking\n",
    "train_ds, test_ds = spatial_blocking(cowy, block_size=2, n_folds=2)\n",
    "\n",
    "# 80/20 train/val split by index\n",
    "# train_idx and val_idx are integer arrays of row positions with train or val_ds.obslookup not cowy.obs_lookup that contrain 1 - len train_ds (len(val_ds))\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(train_ds)),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_ds = copy.deepcopy(train_ds)\n",
    "\n",
    "train_ds.obs_lookup = train_ds.obs_lookup.iloc[train_idx].reset_index(drop=True)\n",
    "val_ds.obs_lookup   = val_ds.obs_lookup.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "#balance training class\n",
    "train_ds = balance_train_classes(train_ds, thresholds=[11.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58961779",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir ='/gscratch/kylabazlen/ml_out/test_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbda027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save obs look ups for each split\n",
    "train_ds.obs_lookup.to_csv(os.path.join(save_dir, \"train_obs_lookup.csv\"), index=False)\n",
    "val_ds.obs_lookup.to_csv(os.path.join(save_dir, \"val_obs_lookup.csv\"), index=False)\n",
    "test_ds.obs_lookup.to_csv(os.path.join(save_dir, \"test_obs_lookup.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a08b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save feature order\n",
    "with open(\"/gscratch/kylabazlen/ml_out/test_1/vars_hrrr.txt\", \"w\") as f:\n",
    "    for var in cowy.vars_hrrr:\n",
    "        f.write(f\"{var}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca1c5c",
   "metadata": {},
   "source": [
    "## Runnnig with presaved arrays ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a633c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/gscratch/kylabazlen/ml_out/test_1/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f0a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedNorm(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        # register buffers so they move with the model (.to(device))\n",
    "        self.register_buffer('mean', torch.as_tensor(mean))\n",
    "        self.register_buffer('std', torch.as_tensor(std))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd39e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCorrectionModel(L.LightningModule):\n",
    "    \"\"\"Custom model that uses nn.Linear layers to take features as input\n",
    "    and predict point observations for bias correction.\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, mean, std, config, dense_weight_obj=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input channels. This will be axis=1 in the input tensor\n",
    "        n_outputs : int\n",
    "            Number of output channels. This will be axis=1 in the output tensor\n",
    "        mean : tensor\n",
    "            Mean for normalization\n",
    "        std : tensor\n",
    "            Standard deviation for normalization\n",
    "        config : dict\n",
    "            Hyperparameters including number of channels in the latent space and optimizer keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.config = config\n",
    "        self.dw = dense_weight_obj\n",
    "        self.n_filters = config['n_filters']\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            FixedNorm(mean, std),\n",
    "            nn.Linear(in_features=n_inputs, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Dropout(p=self.config['drop_out_p']),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Dropout(p=self.config['drop_out_p']),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Dropout(p=self.config['drop_out_p']),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Dropout(p=self.config['drop_out_p']),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=self.n_filters), nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=self.n_filters, out_features=n_outputs),\n",
    "        )\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        # L1 regularization on ALL model parameters\n",
    "        l1_norm = sum(p.abs().sum() for p in self.parameters())\n",
    "        l1_penalty = self.config['l1_lambda'] * l1_norm\n",
    "        total_loss = loss + l1_penalty\n",
    "\n",
    "        self.log(\"train_loss\", total_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validate on batch with RMSE loss\"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self(x) #forward passing\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"validation_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Get AdamW optimizer\"\"\"\n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                         lr = self.config['learning_rate'],\n",
    "                         eps = self.config['eps'],\n",
    "                         weight_decay = self.config['weight_decay'])\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        X = self.model(X)\n",
    "        return X\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"MSE loss + L1 penalty on feature weights\"\"\"\n",
    "        mse_loss = nn.functional.mse_loss(y_pred, y_true)\n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc4da177",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 2500,\n",
    "    'learning_rate': 0.0001,\n",
    "    'eps' : 0.00001,\n",
    "    'max_epochs': 50,\n",
    "    'n_filters': 640,\n",
    "    'weight_decay': 0.0001, #\"L2\" regularization\n",
    "    'drop_out_p': 0.25,\n",
    "    'l1_lambda': 0.000001,\n",
    "}\n",
    "\n",
    "with open(f'{save_dir}config.json', \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad7cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.load(\"/gscratch/kylabazlen/ml_out/test_1/X_all.npy\")\n",
    "Y_all = np.load(\"/gscratch/kylabazlen/ml_out/test_1/Y_all.npy\")\n",
    "X_all = torch.from_numpy(X_all)\n",
    "Y_all = torch.from_numpy(Y_all)\n",
    "\n",
    "mean = np.load(\"/gscratch/kylabazlen/ml_out/test_1/mean.npy\")\n",
    "sd = np.load(\"/gscratch/kylabazlen/ml_out/test_1/sd.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45aaea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in obs lookup files\n",
    "train_ds = pd.read_csv('/gscratch/kylabazlen/ml_out/test_1/train_obs_lookup.csv')\n",
    "test_ds_obs_lookup = pd.read_csv('/gscratch/kylabazlen/ml_out/test_1/test_obs_lookup.csv')\n",
    "val_obs_lookup = pd.read_csv('/gscratch/kylabazlen/ml_out/test_1/val_obs_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730b9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(X_all[train_ds['obs_number'].values], Y_all[train_ds['obs_number'].values])\n",
    "val_ds = TensorDataset(X_all[val_obs_lookup['obs_number'].values], Y_all[val_obs_lookup['obs_number'].values])\n",
    "test_ds = TensorDataset(X_all[test_ds_obs_lookup['obs_number'].values], Y_all[test_ds_obs_lookup['obs_number'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033defae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=True, #shuffle true for training\n",
    "                              num_workers=4) #parallel loading\n",
    "\n",
    "val_dataloader   = DataLoader(val_ds,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=False, \n",
    "                              num_workers=0)\n",
    "\n",
    "#shuffle false for testing to keep alignment with obs lookup\n",
    "test_dataloader  = DataLoader(test_ds,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              shuffle=False,\n",
    "                              num_workers=0)\n",
    "\n",
    "xsample, ysample = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "291362a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = None\n",
    "\n",
    "model = PointCorrectionModel(\n",
    "    n_inputs=len(xsample),\n",
    "    n_outputs=len(ysample),\n",
    "    mean=mean,\n",
    "    std=sd,\n",
    "    config=config,\n",
    "    dense_weight_obj = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbb253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/cowy-nvhackathon/software/cowy-wildfire-envs/cowy-wildfire-env/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /project/cowy-nvhackathon/software/cowy-wildfire-env ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/project/cowy-nvhackathon/software/cowy-wildfire-envs/cowy-wildfire-env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:881: Checkpoint directory /cluster/medbow/gscratch/kylabazlen/ml_out/test_1/lightning_logs/PointCorrectionModel/version_1 exists and is not empty.\n",
      "\n",
      "  | Name  | Type       | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------\n",
      "0 | model | Sequential | 7.4 M  | train | 0    \n",
      "-----------------------------------------------------\n",
      "7.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.642    Total estimated model params size (MB)\n",
      "45        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/cowy-nvhackathon/software/cowy-wildfire-envs/cowy-wildfire-env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1184/1184 [04:42<00:00,  4.20it/s, v_num=1, train_loss_step=1.360, train_loss_epoch=1.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1184/1184 [04:42<00:00,  4.20it/s, v_num=1, train_loss_step=1.360, train_loss_epoch=1.350]\n"
     ]
    }
   ],
   "source": [
    "# loggers\n",
    "tb_logger = TensorBoardLogger(\n",
    "    \"/gscratch/kylabazlen/ml_out/test_1/lightning_logs\", \n",
    "    name=\"PointCorrectionModel\"\n",
    ")\n",
    "# version number tied to TensorBoard logger\n",
    "logger_version = tb_logger.version\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"/gscratch/kylabazlen/ml_out/test_1/lightning_logs\",\n",
    "    name=\"PointCorrectionModel\",\n",
    "    version=logger_version,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", min_delta=0.00, patience=500, verbose=False) #not real early stopping.. \n",
    "model_ckpt_callback = ModelCheckpoint(monitor=\"validation_loss\",\n",
    "                                      dirpath=f'/gscratch/kylabazlen/ml_out/test_1/lightning_logs/PointCorrectionModel/version_{tb_logger.version}/', \n",
    "                                      verbose=False)\n",
    "\n",
    "# Lightning trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    profiler=None,\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[early_stop_callback, model_ckpt_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir='/gscratch/kylabazlen/ml_out/lightning_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, dataloader):\n",
    "    preds = []\n",
    "    obs = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y_hat = model(x)\n",
    "            preds.append(y_hat)\n",
    "            obs.append(y)\n",
    "    \n",
    "    #Concatenate as tensors, then convert to numpy\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    obs = torch.cat(obs, dim=0).numpy()\n",
    "    \n",
    "    return preds, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## WITH OLD MODEL RUN\n",
    "# full_ckpt = ckpt_path  # Use the manually set checkpoint path\n",
    "# version_dir = os.path.dirname(full_ckpt)\n",
    "\n",
    "#  WITH NEW Checkpoint\n",
    "full_ckpt = model_ckpt_callback.best_model_path\n",
    "version_dir = os.path.dirname(full_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(version_dir, \"best_ckpt_results\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b68566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in OR compute and save predictions for a forward pass of the model for each split.\n",
    "# save_dir = os.path.join(version_dir, \"best_ckpt_results\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#pick one file to check\n",
    "check = os.path.join(save_dir, \"pred_train.npy\")\n",
    "\n",
    "if os.path.exists(check):\n",
    "    print(\"Found saved forward-pass results → loading...\")\n",
    "\n",
    "    pred_train = np.load(os.path.join(save_dir, \"pred_train.npy\"))\n",
    "    obs_train  = np.load(os.path.join(save_dir, \"obs_train.npy\"))\n",
    "\n",
    "    pred_val = np.load(os.path.join(save_dir, \"pred_val.npy\"))\n",
    "    obs_val  = np.load(os.path.join(save_dir, \"obs_val.npy\"))\n",
    "\n",
    "    pred_test = np.load(os.path.join(save_dir, \"pred_test.npy\"))\n",
    "    obs_test  = np.load(os.path.join(save_dir, \"obs_test.npy\"))\n",
    "\n",
    "else:\n",
    "    print(\"No saved results → computing and saving...\")\n",
    "\n",
    "    pred_train, obs_train = forward_pass(model, train_dataloader)\n",
    "    pred_val, obs_val     = forward_pass(model, val_dataloader)\n",
    "    pred_test, obs_test   = forward_pass(model, test_dataloader)\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"pred_train.npy\"), pred_train)\n",
    "    np.save(os.path.join(save_dir, \"obs_train.npy\"),  obs_train)\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"pred_val.npy\"), pred_val)\n",
    "    np.save(os.path.join(save_dir, \"obs_val.npy\"),  obs_val)\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"pred_test.npy\"), pred_test)\n",
    "    np.save(os.path.join(save_dir, \"obs_test.npy\"),  obs_test)\n",
    "\n",
    "    print(\"Saved results to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ab174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dataloader):\n",
    "    inputs = []\n",
    "    obs = []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        inputs.append(x)\n",
    "        obs.append(y)\n",
    "    \n",
    "    inputs = torch.cat(inputs, dim=0).numpy()\n",
    "    obs = torch.cat(obs, dim=0).numpy()\n",
    "    \n",
    "    return inputs, obs\n",
    "\n",
    "x_train, obs_train_bl = extract_data(train_dataloader)\n",
    "x_val, obs_val_bl = extract_data(val_dataloader)\n",
    "x_test, obs_test_bl = extract_data(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b58c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/gscratch/kylabazlen/ml_out/test_1\"\n",
    "\n",
    "np.save(os.path.join(save_directory, \"x_train_bl.npy\"), x_train)\n",
    "np.save(os.path.join(save_directory, \"x_val_bl.npy\"), x_val)\n",
    "np.save(os.path.join(save_directory, \"x_test_bl.npy\"), x_test)\n",
    "\n",
    "np.save(os.path.join(save_directory, \"obs_train_bl.npy\"), obs_train_bl)\n",
    "np.save(os.path.join(save_directory, \"obs_val_bl.npy\"), obs_val_bl)\n",
    "np.save(os.path.join(save_directory, \"obs_test_bl.npy\"), obs_test_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bins(pred, obs, thresholds=[5,10]):\n",
    "    pred = pred.flatten()\n",
    "    obs = obs.flatten()\n",
    "    diff = pred - obs\n",
    "\n",
    "    thresholds = sorted(thresholds)\n",
    "    edges = [0] + thresholds + [np.inf]\n",
    "\n",
    "    results = {}\n",
    "    # all ws\n",
    "    ss_res = np.sum(diff**2)\n",
    "    ss_tot = np.sum((obs - obs.mean())**2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "    # all ws\n",
    "    results[\"all\"] = {\n",
    "        \"Count\": len(obs),\n",
    "        \"MBE\": diff.mean(),\n",
    "        \"RMSE\": np.sqrt(np.mean(diff**2)),\n",
    "        \"R2\": r2,\n",
    "    }\n",
    "\n",
    "    # bins\n",
    "    for lo, hi in zip(edges[:-1], edges[1:]):\n",
    "        mask = (obs >= lo) & (obs < hi)\n",
    "        d = diff[mask]\n",
    "        obs_bin = obs[mask]\n",
    "\n",
    "        if d.size == 0:\n",
    "            mbe = np.nan\n",
    "            rmse = np.nan\n",
    "            r2_bin = np.nan\n",
    "        else:\n",
    "            mbe = d.mean()\n",
    "            rmse = np.sqrt(np.mean(d**2))\n",
    "            ss_res_bin = np.sum(d**2)\n",
    "            ss_tot_bin = np.sum((obs_bin - obs_bin.mean())**2)\n",
    "            r2_bin = 1 - (ss_res_bin / ss_tot_bin) if ss_tot_bin > 0 else np.nan\n",
    "\n",
    "        label = f\"{lo}-{hi}\" if hi != np.inf else f\">{lo}\"\n",
    "        results[label] = {\n",
    "            \"Count\": int(mask.sum()),\n",
    "            \"MBE\": mbe,\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2_bin,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_stats_table(results):\n",
    "    \"\"\"\n",
    "    Pretty-print MBE/RMSE/R² bin stats from evaluate_bins().\n",
    "    Expected format:\n",
    "        results = {\n",
    "            'all': {'MBE':..., 'RMSE':..., 'R2':..., 'Count':...},\n",
    "            '0-5': {...},\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Header\n",
    "    print(f\"{'Bin':<8} {'Count':>10} {'MBE':>10} {'RMSE':>10} {'R²':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Print all bins EXCEPT all\n",
    "    for key in results:\n",
    "        if key == \"all\":\n",
    "            continue\n",
    "        stats = results[key]\n",
    "        print(f\"{key:<8} {stats['Count']:>10} {stats['MBE']:>10.5f} {stats['RMSE']:>10.5f} {stats['R2']:>10.5f}\")\n",
    "\n",
    "    # Separator\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Print ALL\n",
    "    stats = results[\"all\"]\n",
    "    print(f\"{'All':<8} {stats['Count']:>10} {stats['MBE']:>10.5f} {stats['RMSE']:>10.5f} {stats['R2']:>10.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [5, 10]\n",
    "train_stats = evaluate_bins(pred_train, obs_train, thresholds=thresholds)\n",
    "print(\"Train Split\")\n",
    "print_stats_table(train_stats)\n",
    "val_stats = evaluate_bins(pred_val, obs_val, thresholds=thresholds)\n",
    "print(\"Validation Split\")\n",
    "print_stats_table(val_stats)\n",
    "test_stats = evaluate_bins(pred_test, obs_test, thresholds=thresholds)\n",
    "print(\"Test Split\")\n",
    "print_stats_table(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/gscratch/kylabazlen/ml_out/test_1/vars_hrrr.txt\", \"r\") as f:\n",
    "    vars_hrrr = [line.strip() for line in f]\n",
    "\n",
    "ws_idx = vars_hrrr.index(\"ws_10\")\n",
    "print(ws_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b24b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline IFS Stats 72-f96\")\n",
    "thresholds = [5,10]\n",
    "train_stats = evaluate_bins(x_train[:, ws_idx], obs_train_bl, thresholds=thresholds)\n",
    "print(\"Train Split\")\n",
    "print_stats_table(train_stats)\n",
    "val_stats = evaluate_bins(x_val[:, ws_idx], obs_val_bl, thresholds=thresholds)\n",
    "print(\"Validation Split\")\n",
    "print_stats_table(val_stats)\n",
    "test_stats = evaluate_bins(x_test[:, ws_idx], obs_test_bl, thresholds=thresholds)\n",
    "print(\"Test Split\")\n",
    "print_stats_table(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c936f9",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# import geopandas as gpd\n",
    "# import cartopy.io.shapereader as shpreader\n",
    "# import numpy as np\n",
    "\n",
    "# dem = xr.open_dataset('/projects/cowy/datasets/terrain_data/terrain_990m/conus_elev_reprojected_wgs84_cowy_990m.nc')\n",
    "# # Data\n",
    "# train_lon = train_ds.obs_lookup['longitude']\n",
    "# train_lat = train_ds.obs_lookup['latitude']\n",
    "# test_lon  = test_ds.obs_lookup['longitude']\n",
    "# test_lat  = test_ds.obs_lookup['latitude']\n",
    "\n",
    "# # Load states\n",
    "# states_shp = shpreader.natural_earth(\n",
    "#     resolution=\"10m\",\n",
    "#     category=\"cultural\",\n",
    "#     name=\"admin_1_states_provinces\"\n",
    "# )\n",
    "# gdf_states = gpd.read_file(states_shp)\n",
    "\n",
    "# # Figure\n",
    "# fig, ax = plt.subplots(figsize=(8, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# # DEM\n",
    "# im = ax.pcolormesh(\n",
    "#     dem['longitude'],\n",
    "#     dem['latitude'],\n",
    "#     dem['HGT'],\n",
    "#     cmap='cubehelix',\n",
    "#     shading='auto',\n",
    "#     transform=ccrs.PlateCarree()\n",
    "# )\n",
    "# plt.colorbar(im, ax=ax, label=\"Elevation (m)\")\n",
    "\n",
    "# # Scatter points\n",
    "# ax.scatter(train_lon, train_lat, s=15, c='blue', alpha=.5, edgecolors='lightgray',linewidth=0.2, label='Train', transform=ccrs.PlateCarree())\n",
    "# ax.scatter(test_lon, test_lat, s=15, c='red', alpha=.5, edgecolors='lightgray',linewidth=0.2, label='Test', transform=ccrs.PlateCarree())\n",
    "\n",
    "# # Plot states\n",
    "# gdf_states.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.8, transform=ccrs.PlateCarree())\n",
    "\n",
    "# # Gridlines\n",
    "# gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "# gl.top_labels = False\n",
    "# gl.right_labels = False\n",
    "\n",
    "# # Map extent\n",
    "# ax.set_extent([\n",
    "#     test_lon.min(), test_lon.max(),\n",
    "#     test_lat.min(),  test_lat.max()\n",
    "# ], crs=ccrs.PlateCarree())\n",
    "\n",
    "# # This sets 1 degree latitude = 1 degree longitude in screen space\n",
    "# lon_range = dem['longitude'].max() - dem['longitude'].min()\n",
    "# lat_range = dem['latitude'].max() - dem['latitude'].min()\n",
    "# ax.set_aspect(lon_range / lat_range)\n",
    "\n",
    "# # Title & legend\n",
    "# ax.set_title(\"Training and Test Set Observations\")\n",
    "# ax.legend(loc=\"upper right\", framealpha=1, facecolor=\"white\", edgecolor=\"black\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
